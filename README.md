# Build a generative model that creates X-ray images of hands

<img src="https://drive.google.com/uc?id=1LubLuuyiJwyDNRd2Wj0vaA0Ek2AtAFxH" width="500"/>

<br>

### **Task**:
Create a generative model, either a VAE or a GANs, train it, and use it to generate 300 new samples (new images) of X-ray images of hands. <br>

Justify your choice (there is a dedicated text cell in the 'clean' notebook. There is no right or wrong here, but the quality of the justification will be assessed.

### **Dataset**:
You have 8000 samples available in the repository under the folder `real_hands` that you can use to train your VAE or your GAN.

### **Deliverables**:
1. `VAE_hands` and `GAN_hands` — folder containting the 300 samples generated with your model. The format should be `jpeg`. <br>


2. `GenAI Models.ipynb` — 'clean' notebook containing the code and supporting explanatory text for:
	- Data preparation
	- Design and implementation of your network of choice (VAE or GAN)
	- Implementation of training loops and any other utility functions you see fit
	- Executed cells for network training with accompanying loss evolution plots (using livelossplot or any other visualisation tool you like). Here use the best hyperparameters you have found. **Important: you will have to train your network from scratch**, so no pre-trained networks allowed (unless you pretrain them yourself and show it in the notebook).
	- Generation of 300 samples by using the trained model (and the code you need to save the generated images in your drive or local machine that you will upload in the repo). The uploading does not need to be automated from the notebook directly, you can save the images any way you want and then put them in the repository by copy-pasting them from your local machine or your Google Drive.


3. `Hypertuning.ipynb` — An 'auxiliary' notebook containting any hyperparameter exploration you have done to decide what are the best hyperparameters to use for your final training. You will include your final training with your choice of hyperparameters in the 'clean' notebook described in point 2.). 


# Classify X-ray images of hands generated by different genrative models

<br>

## **Tasks**

### Task 1
Classify the hand images in the `test_hands` folder using any method you have learned during the module. The classifier should classify test hands using the following labels:

- `0`: real hand
- `1`: VAE hand
- `2`: GAN hand

<br>

### Task2
Answer the two questions at the end of this README file. Answer them in the README itself and commit and push your modified README file to your repository.

<br>



## Deliverables

Your final repository should contain the following:

1. `Classification Result.csv`: a comma-separated value file with the name of the hand in the test set, then a comma, and then the prediction for this hand image. It should look like:

	```
	test_hand_0001.jpeg, 2
	test_hand_0002.jpeg, 0
	...
	```
with the appropriate lable values that your classifier generates. Add this new file to the repo and do not put it inside any subfolder please.

2. `Classifier.ipynb`: a clean notebook with your classifier workflow (similar to the one you created in coursework 1), should include data preparation, network implementation, final training, final evaluation of the test set, writing of the csv file, and any other step you have done that is not related to hyperparameter tunning or network design tests (only steps using your best and final classifier model). 

3. `Hypertuning.ipynb`: as in coursework 1, a notebook where you explore your best hyperparameter values for your final network design and training that you will include in `yourusername_DLcw2_clean.ipynb`.

<br>

## **Task 1 - Classifier**

### Datasets
You will find the following folders in this repository:
- `real_hands`: contains real hands (but feel free to use more from the first coursework repo if you think it is a good idea.
- `VAE_hands`: contains hands generated using VAEs.
- `GAN_hands`: contains hands generared using GANs.
- `test_hands`: contains a mix of real hands, VAE-generated hands, and GAN-generated hands.

Use the four datasets as you see fit to design and implement a classifier to label the images in the `test_hands` folder as:

- real hands (label: `0`)
- VAE-generated hands (label: `1`)
- GAN-generated hands (label: `2`)

You have to implement your network explicitly and train it from randomly initialised weights (no transfer learning), but you can use any architecture you think is useful.

<br>

## **Task 2 - Questions**
Modify this README file to include the answers to the questions below.

### Question 1 [150 words maximum]
If I asked you to do the first coursework again, but now I gave you 5 days and 500 compute units (instead of 100 compute units), what would you do differently to improve your results. In this case, you would be able to use any architecture we have seen in class. Justify your answers well, and do not go over the word limit. You can use bullet points and a maximum of one diagram/figure/table to support your answer.

### ANSWER to question 1:

I would still choose GAN for the following reasons:
- GAN produces more realistic images (important for medical use)
- GAN gives better results but it is computationally expensive and unstable, so can benefit from more time and compute units (I nearly ran out of all 100 compute units last time)

Things I would also make a change:
- Use a deeper architecture (including dropout and batch normalisation) for discriminator and generator to capture complex patterns and structures
- Apply data augmentation during training to improve the model's generalization

<br>

### Question 2 [100 words maximum]
Additionally to the newly defined hypotethical assessment in **Question 1**, if i told you that now the images were 512x512 pixels instead of 32x32, and I gave you a choice between these two extra resources:

**a.** as much compute power as you want, but only have the 10000 images provided. <br>
**b.** as many more images in your dataset as you want, but only have the 500 compute units availabe.

Which one would you choose and why? Again justify your answers and use bullet points or tables as you see fit.

### ANSWER to question 2:

I would choose option a. in this case for the following reasons:
- The quality of model is more important than the quantity of images given (Can use a deeper architecture without the concern of running out of all compute units).
- High computational resources can accelerate the training process, allowing for more iterations and convergence towards a better-performing model.
- Higher-resolution images (512x512) means more details are kept in the image, so simple structure might not be competent for the generating intricate and realistic images
- While choosing option b. might increase the diversity of output and reduce the risk of overfitting becasue of more inputs given, intricate and detailed images derived from option a. are more crucial for medical images
